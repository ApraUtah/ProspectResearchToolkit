[
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials & How-to",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nAnalysis of Cars - an example from quarto\n\n\n\n\n\n\nn/a\n\n\nOct 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nUsing PowerQuery\n\n\n\nPaid tool\n\nExcel\n\n\n\n\n\n\n\nGreg Brooks\n\n\nOct 3, 2025\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Education",
      "Tutorials & How-to"
    ]
  },
  {
    "objectID": "tools/free_tools/software/index.html",
    "href": "tools/free_tools/software/index.html",
    "title": "Prospect Research Toolkit",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "toolkit_tutorials/UsingPowerQuery/index.html",
    "href": "toolkit_tutorials/UsingPowerQuery/index.html",
    "title": "Using PowerQuery",
    "section": "",
    "text": "Power Query is a data connection technology in Excel that allows users to connect, combine, and refine data from various sources.\n\n\n\nData Importing: Pulls data from multiple sources (databases, web pages, files).\nData Transformation: Offers tools to clean and reshape data, such as filtering, merging, and aggregating.\nUser-Friendly Interface: Provides a visual interface for users to apply transformations without coding.\nIntegration: Seamlessly integrates with Excel, allowing transformed data to be loaded into worksheets for analysis.\n\nIn essence, Power Query simplifies data preparation, enabling users to focus on analysis and insights."
  },
  {
    "objectID": "toolkit_tutorials/UsingPowerQuery/index.html#what-is-powerquery",
    "href": "toolkit_tutorials/UsingPowerQuery/index.html#what-is-powerquery",
    "title": "Using PowerQuery",
    "section": "",
    "text": "Power Query is a data connection technology in Excel that allows users to connect, combine, and refine data from various sources.\n\n\n\nData Importing: Pulls data from multiple sources (databases, web pages, files).\nData Transformation: Offers tools to clean and reshape data, such as filtering, merging, and aggregating.\nUser-Friendly Interface: Provides a visual interface for users to apply transformations without coding.\nIntegration: Seamlessly integrates with Excel, allowing transformed data to be loaded into worksheets for analysis.\n\nIn essence, Power Query simplifies data preparation, enabling users to focus on analysis and insights."
  },
  {
    "objectID": "toolkit_tutorials/UsingPowerQuery/index.html#powerquery-vs.-vlookup",
    "href": "toolkit_tutorials/UsingPowerQuery/index.html#powerquery-vs.-vlookup",
    "title": "Using PowerQuery",
    "section": "PowerQuery vs. VLookup",
    "text": "PowerQuery vs. VLookup\n\n\n\n\n\n\n\n\nFeature\nPower Query\nVLOOKUP\n\n\n\n\nPurpose\nData transformation and preparation\nLook up and retrieve specific data values\n\n\nData Sources\nConnects to multiple sources (Excel, SQL, web, etc.)\nLimited to data within the worksheet\n\n\nComplexity\nMore complex with advanced capabilities\nSimpler and straightforward\n\n\nOutput\nCreates tables or queries in Excel\nReturns a single value based on a match\n\n\nFlexibility\nHighly versatile, allows for data shaping\nLess flexible; limited to a single match type\n\n\nUse Cases\nIdeal for ETL processes and bulk data manipulation\nBest for simple lookups within a single dataset\n\n\nMaintenance\nRequires more setup but easier to update over time\nSimple to set up but can be error-prone if data changes"
  },
  {
    "objectID": "toolkit_tutorials/UsingPowerQuery/index.html#use-cases",
    "href": "toolkit_tutorials/UsingPowerQuery/index.html#use-cases",
    "title": "Using PowerQuery",
    "section": "Use-cases",
    "text": "Use-cases\nYour workflow may not require PowerQuery, it may be overkill. But given how much fiddling with Excel formulas can happen, and how that can eat up time, PowerQuery honestly seems like it should be more standard in our field - especially because of how reproducible any given process is - the steps being recorded automatically so they could be reused or made into a template.\n\nAbstrqct Example\nYou have data in your CRM that you want to combine with another dataset you’ve found or created. For example, perhaps you’ve created a custom list of individuals who have won awards, but have not updated each of their individual profiles in the database yet. So you are looking to add this supplemental info to help segment a list in an interesting way.\n\n\nSteps to Achieve This\n\nExport Data from CRM: Start by exporting the relevant data from your CRM into a CSV or Excel format.\nImport Data into Power Query: Open Excel, go to the Data tab, and select “Get Data” to import your CRM data and your custom dataset.\nCombine Datasets: Use Power Query tools to merge or append the datasets based on a common identifier.\nTransform Data: Clean, filter, or reshape the data as necessary.\nLoad the Combined Data: Once satisfied with the combined data, load it back into Excel for analysis or reporting.\n\nMore complex data engineering can be done with PowerQuery, and business intelligence analytics with PowerBI"
  },
  {
    "objectID": "toolkit_tutorials/UsingPowerQuery/index.html#power-query-m-code-for-filtering-job-titles-with-dynamic-keywords",
    "href": "toolkit_tutorials/UsingPowerQuery/index.html#power-query-m-code-for-filtering-job-titles-with-dynamic-keywords",
    "title": "Using PowerQuery",
    "section": "Power Query M Code for Filtering Job Titles with Dynamic Keywords",
    "text": "Power Query M Code for Filtering Job Titles with Dynamic Keywords\nThis document outlines a Power Query M script that dynamically loads keyword lists from parameters and filters job title datasets across multiple sectors. The goal is to identify roles related to Major Gifts while excluding irrelevant ones.\n\nDynamic Keyword Parameters\nInstead of hardcoding the keywords, we define them as parameters:\nlet\n    IncludeKeywords = Text.Split(\"Major Gift,Director Of Development,Development Officer,Major Gift Officer,Fundraiser,Philanthropy Officer,Philanthropy Manager\", \",\"),\n    ExcludeKeywords = Text.Split(\"Prospect Research,Event\", \",\")\n\n\nLoad and Filter Nonprofit Dataset\nWe load the nonprofit dataset and filter it using the dynamic keyword lists:\n    Source = Csv.Document(File.Contents(\"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198385_nonprofit.csv\"), [Delimiter=\",\", Columns=7, Encoding=65001, QuoteStyle=QuoteStyle.None]),\n    PromotedHeaders = Table.PromoteHeaders(Source, [PromoteAllScalars=true]),\n    ChangedType = Table.TransformColumnTypes(PromotedHeaders, {\n        {\"sector\", type text}, \n        {\"title_cleaned\", type text}, \n        {\"frequency_cleaned\", Int64.Type}, \n        {\"title_generalized\", type text}, \n        {\"frequency_generalized\", Int64.Type}, \n        {\"title_simplified\", type text}, \n        {\"frequency_simplified\", Int64.Type}\n    }),\n    IncludedRows = Table.SelectRows(ChangedType, each List.AnyTrue(List.Transform(IncludeKeywords, (kw) =&gt; Text.Contains([title_cleaned], kw)))),\n    FinalFilteredRows = Table.SelectRows(IncludedRows, each List.AllTrue(List.Transform(ExcludeKeywords, (kw) =&gt; not Text.Contains([title_cleaned], kw)))),\n    TitleList = Table.SelectColumns(FinalFilteredRows, {\"title_cleaned\"}),\n\n\nReusable Filtering Function\nWe define a reusable function to apply the same filtering logic to other datasets:\n    FilterDataset = (filePath as text) =&gt;\n        let\n            Source = Csv.Document(File.Contents(filePath), [Delimiter=\",\", Columns=7, Encoding=65001, QuoteStyle=QuoteStyle.None]),\n            Promoted = Table.PromoteHeaders(Source, [PromoteAllScalars=true]),\n            ChangedType = Table.TransformColumnTypes(Promoted, {\n                {\"sector\", type text}, \n                {\"title_cleaned\", type text}, \n                {\"frequency_cleaned\", Int64.Type}, \n                {\"title_generalized\", type text}, \n                {\"frequency_generalized\", Int64.Type}, \n                {\"title_simplified\", type text}, \n                {\"frequency_simplified\", Int64.Type}\n            }),\n            Matched = Table.Join(ChangedType, \"title_cleaned\", TitleList, \"title_cleaned\", JoinKind.Inner)\n        in\n            Matched,\n\n\nDataset Paths and Execution\nWe list all dataset paths and apply the filtering function:\n    DatasetPaths = {\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198352_travel_and_tourism.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198376_transportation_and_logistics.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55197857_restaurants_bars_and_food_services.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198349_real_estate.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198379_telecommunications.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198388_oil_gas_energy_and_utilities.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198394_retail.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198373_media.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198403_manufacturing.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198358_insurance.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198406_information_technology.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198382_government.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198397_health_care.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198391_education.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198400_finance.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198190_consumer_services.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198364_construction_repair_and_maintenance.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198409_business_services.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55197803_agriculture_and_forestry.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198355_arts_entertainment_and_recreation.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198370_biotech_and_pharmaceuticals.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198361_aerospace_and_defense.csv\",\n        \"..\\jobtitlesdataset\\Figure 4\\dataset\\titles\\map\\55198367_accounting_and_legal.csv\"\n    },\n    FilteredTables = List.Transform(DatasetPaths, each FilterDataset(_)),\n    CombinedResults = Table.Combine(FilteredTables)\nin\n    CombinedResults\n\n\nSummary\nThis script dynamically loads keyword lists and applies consistent filtering across multiple datasets. It improves maintainability and scalability by avoiding hardcoded values."
  },
  {
    "objectID": "toolkit_blog/PRSPCT-L_roundup/index.html",
    "href": "toolkit_blog/PRSPCT-L_roundup/index.html",
    "title": "unfinished PRSPCT-L Roundup",
    "section": "",
    "text": "A compilation of some of my favorite resources / comments / things I’ve learned from PRSPCT-L from 2025. (still working on this!)\n\nReferences\nI’ve included links to the posts, so you can follow the original conversation. Not sure how folks would like me to attribute their contributions, so in lieu of posting names / job titles here, I figure folks can follow the links and log into PRSPCT-L to thank folks directly. In every case, I am transforming the content somehow, not directly quoting.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "toolkit_blog/Creating_Jobs_Dataset/index.html",
    "href": "toolkit_blog/Creating_Jobs_Dataset/index.html",
    "title": "Creating the Fundraising Jobs Dataset (round 1)",
    "section": "",
    "text": "Perhaps this has already been done - but I’ve been working on creating a fundraising job title and job description dataset. Right now it is scattered, has some irrelevant data, across a few in-progress attempts, which I’ll list below. I have a very small private dataset of recent job titles / descriptions, but the most comprehensive info exists within a recently published public dataset called CMAP.\n\n\n\n\n\n\nTotal Records: 430\nUnique Job Titles: 113\nUnique Sectors: 22\n\n\n\n\n\n\n\n\n```\n\n\n\n…\n\n\n\n\n\n\n\n\n\nI’ve attempted to subset Prospect Development and Major Gift Officer job titles from the 2025 Career Map (CMap) dataset released in July 2025 (citation below) from the CMAP readme.\nQuoting from the paper:\n“We generated our dataset by utilizing a collection of 220 million anonymized and publicly available user curriculum vitae (CVs), collected from LinkedIn via DataHut24. These CVs encompass a total of 546 million job experiences spanning 197 countries and 24 industry sectors. While the dataset itself was collected in 2017, the career histories recorded within these CVs extend as far back as 1970, capturing job trajectories of individuals whose professional experiences span multiple decades, up to December 2017.\nHowever, given the noisy nature of the data, substantial pre-processing was required to ensure consistency and usability. The job titles appeared in a variety of formats, often containing spelling variations, abbreviations, or redundant information. Furthermore, sector classifications were inferred rather than explicitly provided, requiring additional processing to associate each job experience with a standardized industry category This repository provides a comprehensive and scalable dataset for analyzing job titles, promotions, and sector specialization across 24 major industry sectors and 197 countries. It is designed for research in career mobility, labor economics, workforce analytics, and computational social science.”\nTangent: You’ll notice that “Prospect Researcher” is the most common job title, by far, for that niche, and ends up becoming their standardized term for that role at a junior level. However, as soon as you add the word “analyst” to the title (e.g. Senior Prospect Research Analyst), that word becomes the standard - so Senior Prospect Research Analysts become standardized into Senior Analysts. This was useful for their analysis, but to properly understand how many “Prospect Researchers” there are - those aggregates will have to be recalculated using a more ecclectic definition. However, given the massive overlap between these niche Prospect Dev roles, perhaps the best aggregate is actually “Prospect Development Professional” to get more accurate totals from the dataset.\n\n\n\n\nThere are some major caveats to both my datasets created: the Prospect Development job title one, and the Major Gift Officer one. Namely, I was struggling to filter the specific titles that match both of those niche’s, something especially difficult given the variety of titles especially for Major Gift Officers.\nI have included the PowerQuery M code in this repository, in case folks want to see my clumsy attempts to subset CMAP.\nI created a smaller dataset for the MGO niche, that only includes the non-profit sector. If you sort by frequency, you’ll see familiar titles at the top. The “title_generalized” column is arguably the most helpful, because it attempts to standardize the highly variable development officer roles into several categories.\nIn my dataset filenames, I have been calling this CMAP 2017, to remind me that this is somewhat old data. But because of how comprehensive this data is, and because the methods and code are open-source, future dataset creation could follow in the same vein: the main limiting factor being the cost of purchasing public and anonymized CV data. There may be more recent public sector-specific datasets out there that could be appended to this one, or transformed to match their unique standardization system, and potentially avoid this cost.\nI have not uploaded the sector files or massive original dataset, to save on repo space, but they can be downloaded here.\n\n\n\n\nDirect links, if you want to download my filtered data from GitHub as .csv: - Prospect Development job titles - Major Gift Officer nonprofit sector titles - Major Gift Officer all sector titles - full of irrelevant titles right now, until I filter it better\n\n\n\n\nCitation: Subhani, S., Memon, S.A. & AlShebli, B. CMap: a database for mapping job titles, sector specialization, and promotions across 24 sectors. Sci Data 12, 1214 (2025). https://doi.org/10.1038/s41597-025-05526-3"
  },
  {
    "objectID": "toolkit_blog/Creating_Jobs_Dataset/index.html#cmap-prospect-research-subset",
    "href": "toolkit_blog/Creating_Jobs_Dataset/index.html#cmap-prospect-research-subset",
    "title": "Creating the Fundraising Jobs Dataset (round 1)",
    "section": "",
    "text": "Total Records: 430\nUnique Job Titles: 113\nUnique Sectors: 22\n\n\n\n\n\n\n\n\n```\n\n\n\n…"
  },
  {
    "objectID": "toolkit_blog/Creating_Jobs_Dataset/index.html#cmap-2017",
    "href": "toolkit_blog/Creating_Jobs_Dataset/index.html#cmap-2017",
    "title": "Creating the Fundraising Jobs Dataset (round 1)",
    "section": "",
    "text": "I’ve attempted to subset Prospect Development and Major Gift Officer job titles from the 2025 Career Map (CMap) dataset released in July 2025 (citation below) from the CMAP readme.\nQuoting from the paper:\n“We generated our dataset by utilizing a collection of 220 million anonymized and publicly available user curriculum vitae (CVs), collected from LinkedIn via DataHut24. These CVs encompass a total of 546 million job experiences spanning 197 countries and 24 industry sectors. While the dataset itself was collected in 2017, the career histories recorded within these CVs extend as far back as 1970, capturing job trajectories of individuals whose professional experiences span multiple decades, up to December 2017.\nHowever, given the noisy nature of the data, substantial pre-processing was required to ensure consistency and usability. The job titles appeared in a variety of formats, often containing spelling variations, abbreviations, or redundant information. Furthermore, sector classifications were inferred rather than explicitly provided, requiring additional processing to associate each job experience with a standardized industry category This repository provides a comprehensive and scalable dataset for analyzing job titles, promotions, and sector specialization across 24 major industry sectors and 197 countries. It is designed for research in career mobility, labor economics, workforce analytics, and computational social science.”\nTangent: You’ll notice that “Prospect Researcher” is the most common job title, by far, for that niche, and ends up becoming their standardized term for that role at a junior level. However, as soon as you add the word “analyst” to the title (e.g. Senior Prospect Research Analyst), that word becomes the standard - so Senior Prospect Research Analysts become standardized into Senior Analysts. This was useful for their analysis, but to properly understand how many “Prospect Researchers” there are - those aggregates will have to be recalculated using a more ecclectic definition. However, given the massive overlap between these niche Prospect Dev roles, perhaps the best aggregate is actually “Prospect Development Professional” to get more accurate totals from the dataset.\n\n\n\n\nThere are some major caveats to both my datasets created: the Prospect Development job title one, and the Major Gift Officer one. Namely, I was struggling to filter the specific titles that match both of those niche’s, something especially difficult given the variety of titles especially for Major Gift Officers.\nI have included the PowerQuery M code in this repository, in case folks want to see my clumsy attempts to subset CMAP.\nI created a smaller dataset for the MGO niche, that only includes the non-profit sector. If you sort by frequency, you’ll see familiar titles at the top. The “title_generalized” column is arguably the most helpful, because it attempts to standardize the highly variable development officer roles into several categories.\nIn my dataset filenames, I have been calling this CMAP 2017, to remind me that this is somewhat old data. But because of how comprehensive this data is, and because the methods and code are open-source, future dataset creation could follow in the same vein: the main limiting factor being the cost of purchasing public and anonymized CV data. There may be more recent public sector-specific datasets out there that could be appended to this one, or transformed to match their unique standardization system, and potentially avoid this cost.\nI have not uploaded the sector files or massive original dataset, to save on repo space, but they can be downloaded here.\n\n\n\n\nDirect links, if you want to download my filtered data from GitHub as .csv: - Prospect Development job titles - Major Gift Officer nonprofit sector titles - Major Gift Officer all sector titles - full of irrelevant titles right now, until I filter it better\n\n\n\n\nCitation: Subhani, S., Memon, S.A. & AlShebli, B. CMap: a database for mapping job titles, sector specialization, and promotions across 24 sectors. Sci Data 12, 1214 (2025). https://doi.org/10.1038/s41597-025-05526-3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prospect Research Toolkit",
    "section": "",
    "text": "…\n\n\nVersion: 0.1.1-alpha\n\nHowdy, partner. Welcome to camp!\nYou know how they say, “there’s gold in them thar hills?” - well, I’ve set out to create the resource I always wish existed: a free, community-driven library of tools for the scrappy fundraising prospector, along with a field guide to put all of them into practice.\nI hope this toolkit will expand to be an invaluable resource for those splashing through the data streams, Excel sluice boxes and CRM picks a’clankin’!\nFor more info about the who, what, where, and why…feel free to check out the About page.\nThanks for reading and happy trails.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data/readme.html",
    "href": "data/readme.html",
    "title": "Prospect Research Toolkit",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data/job_titles/readme.html",
    "href": "data/job_titles/readme.html",
    "title": "Prospect Research Toolkit",
    "section": "",
    "text": "Working on expanding the fundraising sector job titles dataset, using data gathered myself, and cross-comparing with public datasets like CMap\n\n\n\n Back to top"
  },
  {
    "objectID": "data/job_titles/CMAP/figure_4.html",
    "href": "data/job_titles/CMAP/figure_4.html",
    "title": "Prospect Research Toolkit",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport glob\nimport matplotlib.gridspec as gridspec\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\n\ndef read_files(folder, add_file_column=False, **kwargs):\n    def read_file(path, add_file_column=False, **kwargs):\n        file = os.path.basename(path)\n        try:\n            df = pd.read_csv(path, low_memory=False, **kwargs)\n        except pd.errors.EmptyDataError:\n            print(f\"{file} is empty\")\n            return pd.DataFrame()\n\n        if add_file_column:\n            df['file'] = file\n\n        return df\n\n    return pd.concat([read_file(path, add_file_column, **kwargs) for path in tqdm(glob.glob(os.path.join(folder, '*.csv')))], ignore_index=True)\n\n\ndf_titles_map = read_files('dataset/titles/map')\ndf_titles_si = read_files('dataset/titles/si')\ndf_promotions_unvalidated = pd.concat([read_files('dataset/promotions/unvalidated/edges')], ignore_index=True)\ndf_promotions_validated = pd.concat([read_files('dataset/promotions/validated/edges')], ignore_index=True)\n\n100%|██████████| 24/24 [00:10&lt;00:00,  2.33it/s]\n100%|██████████| 24/24 [00:00&lt;00:00, 27.58it/s]\n100%|██████████| 144/144 [00:00&lt;00:00, 480.33it/s]\n100%|██████████| 48/48 [00:00&lt;00:00, 480.23it/s]\n\n\n\n# Recreate df_sector_counts\ndf_sector_counts = df_titles_map.groupby('sector')['title_generalized'].nunique().reset_index().rename(columns={'title_generalized': 'count'})\ndf_sector_counts['percentage'] = (df_sector_counts['count'] / df_sector_counts['count'].sum()) * 100\n\n# Recreate df_sector_promotions\ndf_sector_promotions_unvalidated = df_promotions_unvalidated.groupby('sector').size().reset_index(name='count')\ndf_sector_promotions_unvalidated['percentage'] = (df_sector_promotions_unvalidated['count'] / df_sector_promotions_unvalidated['count'].sum()) * 100\n\ndf_sector_promotions_validated = df_promotions_validated.groupby('sector').size().reset_index(name='count')\ndf_sector_promotions_validated['percentage'] = (df_sector_promotions_validated['count'] / df_sector_promotions_validated['count'].sum()) * 100\n\n# Recreate df_grouped (for world map)\ndf_grouped = df_promotions_unvalidated.groupby('country_binned').size().reset_index(name='count')\ndf_grouped['percentage'] = (df_grouped['count'] / df_grouped['count'].sum()) * 100\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\n\n# Create a figure with a gridspec layout\nfig = plt.figure(figsize=(10, 16))\ngs = gridspec.GridSpec(4, 1, height_ratios=[2, 3, 2, 2], figure=fig)\n\n# Create axes for each subplot using gridspec\nax1 = fig.add_subplot(gs[0])  # Bar plot of unique titles per sector\nax2 = fig.add_subplot(gs[1])  # Boxplot of SI per sector\nax3 = fig.add_subplot(gs[2])  # Bar plot of promotions (unvalidated)\nax4 = fig.add_subplot(gs[3])  # Bar plot of promotions (validated)\n\n# --- Plot 1: Bar plot of unique titles per sector ---\nsns.barplot(data=df_sector_counts, x='sector', y='count', ax=ax1, color='tab:blue')\nax1.set_ylabel(\"Number of titles\", fontsize=10)\n\n# Add percentage labels on top of each bar\nfor index, row in enumerate(df_sector_counts.itertuples()):\n    percentage_text = f\"{int(row.percentage)}%\" if row.percentage &gt; 9 else f\"{row.percentage:.1f}%\"\n    ax1.text(index, row.count + 2500, percentage_text, ha='center', fontsize=9)\n\n# Format y-axis to show count in thousands (K format)\nax1.set_yticks(ax1.get_yticks())\nax1.set_yticklabels([f\"{int(y/1e3)}K\" for y in ax1.get_yticks()])\n\n# Remove x-axis labels and ticks completely\nax1.set_xticklabels([])\nax1.set_xticks([])\n\nsns.despine(ax=ax1)\nax1.set_xlabel(\"\")\n\n# --- Plot 2: Boxplot of Specialization Index (SI) per sector ---\nsns.boxplot(\n    data=df_titles_si, x='sector', y='SI', ax=ax2, showmeans=True,\n    meanline=False, meanprops={'marker': 'D', 'markerfacecolor': 'black', 'markeredgecolor': 'black', 'markersize': 4},\n    whis=[0, 100]\n)\n\n# Draw the horizontal mean line\noverall_mean = df_titles_si['SI'].mean()\nax2.axhline(overall_mean, linestyle='dotted', color='red', linewidth=1.5)\nax2.set_ylabel(\"Specialization Index (SI)\\n\", fontsize=10)\n\n# Rotate x-axis labels for readability\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\nax2.set_xlabel(\"\")\n\nsns.despine(ax=ax2)\n\n# --- Plot 3: Bar plot of promotions (Validated) ---\nsns.barplot(data=df_sector_promotions_validated, x='sector', y='count', ax=ax3, color='tab:blue')\nax3.set_ylabel(\"Number of promotions\\n\", fontsize=10)\n\n# Add percentage labels on top of each bar\nfor index, row in enumerate(df_sector_promotions_validated.itertuples()):\n    percentage_text = f\"{int(row.percentage)}%\" if row.percentage &gt; 9 else f\"{row.percentage:.1f}%\"\n    ax3.text(index, row.count + 400, percentage_text, ha='center', fontsize=9)\n\n# Format y-axis to show count in thousands (K format)\nax3.set_yticks(ax3.get_yticks())\nax3.set_yticklabels([f\"{int(y/1e3)}K\" for y in ax3.get_yticks()])\n\n# Remove x-axis labels and ticks completely\nax3.set_xticklabels([])\nax3.set_xticks([])\n\nsns.despine(ax=ax3)\nax3.set_xlabel(\"\")\n\n# --- Plot 4: Bar plot of promotions (Unvalidated) ---\nsns.barplot(data=df_sector_promotions_unvalidated, x='sector', y='count', ax=ax4, color='tab:blue')\nax4.set_ylabel(\"Number of promotions\\n\", fontsize=10)\n\n# Add percentage labels on top of each bar\nfor index, row in enumerate(df_sector_promotions_unvalidated.itertuples()):\n    percentage_text = f\"{int(row.percentage)}%\" if row.percentage &gt; 9 else f\"{row.percentage:.1f}%\"\n    ax4.text(index, row.count + 1500, percentage_text, ha='center', fontsize=9)\n\n# Format y-axis to show count in thousands (K format)\nax4.set_yticks(ax4.get_yticks())\nax4.set_yticklabels([f\"{int(y/1e3)}K\" for y in ax4.get_yticks()])\n\n# Rotate x-axis labels for readability\nax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n\n# Rotate x-axis labels for readability\nax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n\nsns.despine(ax=ax4)\nax4.set_xlabel(\"\")\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the combined plot\nplt.show()\n\n# Save in high resolution\nfig.savefig('figure_4.png', dpi=600)\n\nC:\\Users\\Shehryar\\AppData\\Local\\Temp\\ipykernel_39828\\2159619807.py:48: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\nC:\\Users\\Shehryar\\AppData\\Local\\Temp\\ipykernel_39828\\2159619807.py:87: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\nC:\\Users\\Shehryar\\AppData\\Local\\Temp\\ipykernel_39828\\2159619807.py:90: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Toolkit Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nCreating the Fundraising Jobs Dataset (round 1)\n\n\n\n\n\n\nGreg Brooks\n\n\nOct 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nunfinished PRSPCT-L Roundup\n\n\n\n\n\n\nGreg Brooks\n\n\nOct 4, 2025\n\n\n\n\n\n\n\n\n\n\n\nTechnical Field Guide\n\n\n\n\n\n\nGreg Brooks\n\n\nOct 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nLet’s talk data privacy\n\n\n\n\n\n\nGreg Brooks\n\n\nOct 1, 2025\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Home",
      "Education",
      "Toolkit Blog"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "…\nThis toolkit is maintained by Greg Brooks in association with Apra Utah."
  },
  {
    "objectID": "about.html#why-does-this-exist",
    "href": "about.html#why-does-this-exist",
    "title": "About",
    "section": "Why does this exist?",
    "text": "Why does this exist?\nThere are plenty of excellent paid resources available: Apra, private consultants, subscription tools, books, but not everyone has the budget to access them. This toolkit is being created to bridge that gap, or supplement what’s already out there, not to replace any of those things. The goal is to keep it simple, open-source, community-driven, free and regularly updated."
  },
  {
    "objectID": "about.html#who-is-this-guy-anyway",
    "href": "about.html#who-is-this-guy-anyway",
    "title": "About",
    "section": "Who is this guy anyway?",
    "text": "Who is this guy anyway?\nI come from a psychology background and am largely self-taught in data analytics, information science, data science, etc. So keep an eye out for mistakes, and feel free to let me by raising an issue on our (Github issue page)[https://github.com/ApraUtah/ProspectResearchToolkit/issues]."
  },
  {
    "objectID": "about.html#how-often-is-the-website-updated",
    "href": "about.html#how-often-is-the-website-updated",
    "title": "About",
    "section": "How often is the website updated?",
    "text": "How often is the website updated?\nIn small ways, frequently! Larger changes and resources are on a slower schedule, since I’m doing this in my free time. I expect to have more tutorials, resources, and tools online by 2026. Click here to see the features / goals currently being worked on!"
  },
  {
    "objectID": "about.html#how-is-apra-utah-involved",
    "href": "about.html#how-is-apra-utah-involved",
    "title": "About",
    "section": "How is Apra Utah involved?",
    "text": "How is Apra Utah involved?\nI started this website as part of my education initiative for 2025 while I’m serving on the Apra Utah board. In the future (e.g. when I figure out how to do it without breaking too many things) the repository and website name will be changed to a more general one, with participating Apra chapters listed on the website’s About page. The dream would be to have an open-source collective of contributors / volunteers who help maintain the project and add to its longevity, since right now it is almost entirely a one-man show."
  },
  {
    "objectID": "data/job_titles/CMAP/cleanCMAP_dataset/readme.html",
    "href": "data/job_titles/CMAP/cleanCMAP_dataset/readme.html",
    "title": "Prospect Research Toolkit",
    "section": "",
    "text": "Used PowerQuery to attempt to filter out various fundraising specific job titles from the CMAP data (Nature paper on this dataset was published in 2025).\n\n3500+ instances of 100+ job titles in the prospect development dataset, across all sectors. Curious how close that is to an actual workforce estimate for prospect dev as a field in 2017. Almost certainly an undercount, but by how much? And how has that changed in 2025?\nGoing to add a front-line fundraiser dataset soon\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data/job_titles/CMAP/readme.html",
    "href": "data/job_titles/CMAP/readme.html",
    "title": "Greg’s Overview",
    "section": "",
    "text": "I’ve attempted to subset Prospect Development and Major Gift Officer job titles from the 2025 Career Map (CMap) dataset released in July 2025 (citation below) from the CMAP readme:\n\nQuoting from the (paper)[https://www.nature.com/articles/s41597-025-05526-3#Abs1]: # Career Map Dataset (CMap)\n“We generated our dataset by utilizing a collection of 220 million anonymized and publicly available user curriculum vitae (CVs), collected from LinkedIn via DataHut24. These CVs encompass a total of 546 million job experiences spanning 197 countries and 24 industry sectors. While the dataset itself was collected in 2017, the career histories recorded within these CVs extend as far back as 1970, capturing job trajectories of individuals whose professional experiences span multiple decades, up to December 2017. However, given the noisy nature of the data, substantial pre-processing was required to ensure consistency and usability. The job titles appeared in a variety of formats, often containing spelling variations, abbreviations, or redundant information. Furthermore, sector classifications were inferred rather than explicitly provided, requiring additional processing to associate each job experience with a standardized industry category” This repository provides a comprehensive and scalable dataset for analyzing job titles, promotions, and sector specialization across 24 major industry sectors and 197 countries. It is designed for research in career mobility, labor economics, workforce analytics, and computational social science.\n\nSo there are some major caveats to both my datasets created: the Prospect Development job title one, and the Major Gift Officer one. Namely, I was struggling to filter the specific titles that match both of those niche’s, something especially difficult given the variety of titles especially for Major Gift Officers.\nI have included the PowerQuery M code in this repository, in case folks want to see my clumsy attempts to subset CMAP.\nI created a smaller dataset for the MGO niche, that only includes the non-profit sector. If you sort by frequency, you’ll see familiar titles at the top. The “title_generalized” column is arguably the most helpful, because it attempts to standardize the highly variable development officer roles into several categories.\nIn my dataset filenames, I have been calling this CMAP 2017, to remind me that this is somewhat old data. But because of how comprehensive this data is, and because the methods and code are open-source, future dataset creation could follow in the same vein: the main limiting factor being the cost of purchasing public and anonymized CV data. There may be more recent public sector-specific datasets out there that could be appended to this one, or transformed to match their unique standardization system, and potentially avoid this cost.\nI have not uploaded the sector files or massive original dataset, to save on repo space, but they can be downloaded here. ## Archive Overview\nCitation: Subhani, S., Memon, S.A. & AlShebli, B. CMap: a database for mapping job titles, sector specialization, and promotions across 24 sectors. Sci Data 12, 1214 (2025). https://doi.org/10.1038/s41597-025-05526-3 The dataset is split into three downloadable archives, each serving a different purpose:"
  },
  {
    "objectID": "data/job_titles/CMAP/readme.html#job-promotions-and-titles-dataset",
    "href": "data/job_titles/CMAP/readme.html#job-promotions-and-titles-dataset",
    "title": "Greg’s Overview",
    "section": "Job Promotions and Titles Dataset",
    "text": "Job Promotions and Titles Dataset\n\n1. dataset.zip\nContains the core dataset, structured into the following directories:\nThis repository contains structured datasets related to job promotions, title mappings, and sector specializations across various industries. The data is organized into directories based on its purpose and validation status. #### titles/ - map/: Cleaned and generalized job title mappings for each of the 24 sectors. - si/: Specialization Index (SI) scores for job titles by sector, decomposed into: - Sector Breadth (SB): Measures how widely a job title is distributed across sectors. - Sector Depth (SD): Measures how dominant a job title is within a sector."
  },
  {
    "objectID": "data/job_titles/CMAP/readme.html#directory-structure",
    "href": "data/job_titles/CMAP/readme.html#directory-structure",
    "title": "Greg’s Overview",
    "section": "Directory Structure",
    "text": "Directory Structure\n\npromotions/\n\nvalidated/: Empirically validated job promotions across sectors in the US and UK.\nunvalidated/: Statistically inferred promotions across six regions, unvalidated manually.\nnodes/: Title-level job counts associated with promotion movements.\nedges/: Directed job transitions labeled with promotion probabilities.\n\ndataset/ │── promotions/ │ │── validated/ # Contains validated job promotion data │ │── unvalidated/ # Contains unvalidated job promotion data │── titles/ │ │── map/ # Contains mappings of job titles before and after standardization │ │── si/ # Contains specialization index (SI) scores for job titles #### network/ - Interactive HTML visualizations of sector-specific job promotion networks by country/region.\n\n\n1. promotions/\nThis directory contains job movements identified as promotions. The data is divided into: - validated/: Contains human-validated promotions for job movements across different sectors in the United States and United Kingdom. - unvalidated/: Contains statistically inferred promotions for job movements across six continents, but without manual validation. —\nEach CSV file follows the naming convention: ### 2. dataset_ext.zip Provides an extended dataset for advanced modeling and custom data cleaning applications.\n&lt;REGION&gt;_\\&lt;sector&gt;.csv #### map-raw/ - Contains a step-by-step mapping of 56.4 million titles (after processing step J2) to 126 thousand standardized titles (as in step J7 of the manuscript). - The files are split into 100 parts for scalability. - This extended mapping is ideal for: - Matching messy job titles in external datasets (e.g., LinkedIn, Glassdoor, Indeed). - Training supervised learning models for job title normalization using techniques such as: - Logistic regression - Support Vector Machines (SVM) - Transformer-based models (e.g., BERT, RoBERTa)\n\n\n2. titles/"
  },
  {
    "objectID": "data/job_titles/CMAP/readme.html#this-directory-contains-information-about-job-titles-their-frequencies-and-sector-specialization.",
    "href": "data/job_titles/CMAP/readme.html#this-directory-contains-information-about-job-titles-their-frequencies-and-sector-specialization.",
    "title": "Greg’s Overview",
    "section": "This directory contains information about job titles, their frequencies, and sector specialization.",
    "text": "This directory contains information about job titles, their frequencies, and sector specialization.\n\nmap/: Contains mappings of job titles before and after cleaning and standardization for different sectors.\nsi/: Contains Specialization Index (SI) scores, which quantify how unique or general a job title is within a sector. ### 3. examples.zip Includes hand-picked examples used throughout the manuscript for:\nAnnotated Prolific validation tasks\nVisual illustrations of SI distributions and promotion transitions\nComparative examples between US and UK sectors"
  },
  {
    "objectID": "data/job_titles/CMAP/readme.html#dataset-highlights",
    "href": "data/job_titles/CMAP/readme.html#dataset-highlights",
    "title": "Greg’s Overview",
    "section": "Dataset Highlights",
    "text": "Dataset Highlights\n\nSource: 227 million anonymized CVs across 197 countries (1970–2017)\nScope: 572 million job experiences, 691,000+ unique cleaned job titles\nSector Coverage: 24 industry sectors, from Health Care to Information Technology\nPromotion Records: Over 310,000 transitions labeled with promotion probabilities\nO*NET Integration: Titles are mapped to standardized O*NET SOC occupation codes\n\n\nEach CSV file in the above folders follows the naming convention: ## Suggested Applications\n&lt;sector&gt;.csv - Labor market segmentation, job taxonomy construction, promotion inequality analysis - Creation of custom job title cleaning models - Enrichment of external datasets using structured title mappings and SI scores"
  },
  {
    "objectID": "data/job_titles/CMAP/readme.html#usage",
    "href": "data/job_titles/CMAP/readme.html#usage",
    "title": "Greg’s Overview",
    "section": "Usage",
    "text": "Usage\nThis dataset can be used for various research purposes, including: - Analyzing career progression trends across sectors and regions. - Understanding job title variations and sector specializations. - Developing machine learning models for job transition predictions. - Developng generalized models for cleaning job titles. For implementation examples and integration tips, refer to the Usage Notes section in the accompanying manuscript.\n\nLast updated: 4/22/25"
  },
  {
    "objectID": "data/NA_donation_sample.html",
    "href": "data/NA_donation_sample.html",
    "title": "Donated Money Data",
    "section": "",
    "text": "Donated Money Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeography\nTime\nDemographic\nDemographic Value\nYes\nNo\nDK/RF\nN Size\n\n\n\n\nNorthern America\n2024\nAggregate\nAggregate\n55%\n45%\n0%\n2,024\n\n\nNorthern America\n2023\nAggregate\nAggregate\n61%\n39%\n0%\n2,009\n\n\nNorthern America\n2022\nAggregate\nAggregate\n62%\n38%\n0%\n2,017\n\n\nNorthern America\n2021\nAggregate\nAggregate\n61%\n39%\n0%\n2,013\n\n\nNorthern America\n2020\nAggregate\nAggregate\n45%\n55%\n0%\n2,013\n\n\nNorthern America\n2019\nAggregate\nAggregate\n56%\n44%\n0%\n2,057\n\n\nNorthern America\n2018\nAggregate\nAggregate\n53%\n47%\n0%\n2,013\n\n\nNorthern America\n2017\nAggregate\nAggregate\n61%\n39%\n0%\n2,018\n\n\nNorthern America\n2016\nAggregate\nAggregate\n56%\n44%\n0%\n2,048\n\n\nNorthern America\n2015\nAggregate\nAggregate\n63%\n37%\n0%\n1,281\n\n\nNorthern America\n2014\nAggregate\nAggregate\n63%\n36%\n0%\n2,048\n\n\nNorthern America\n2013\nAggregate\nAggregate\n68%\n31%\n0%\n1,092\n\n\nNorthern America\n2012\nAggregate\nAggregate\n63%\n37%\n0%\n2,076\n\n\nNorthern America\n2011\nAggregate\nAggregate\n58%\n42%\n0%\n1,274\n\n\nNorthern America\n2010\nAggregate\nAggregate\n65%\n35%\n—\n2,012\n\n\nNorthern America\n2009\nAggregate\nAggregate\n60%\n39%\n1%\n2,014\n\n\nNorthern America\n2008\nAggregate\nAggregate\n66%\n34%\n0%\n2,009\n\n\nNorthern America\n2007\nAggregate\nAggregate\n61%\n39%\n0%\n2,235\n\n\nNorthern America\n2006\nAggregate\nAggregate\n64%\n36%\n—\n1,355\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "free_resources_links.html",
    "href": "free_resources_links.html",
    "title": "Free External Resources",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Home",
      "External Resources",
      "Free External Resources"
    ]
  },
  {
    "objectID": "templates.html",
    "href": "templates.html",
    "title": "Templates",
    "section": "",
    "text": "Excel templates\n\n\nother templates (?)\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Tools",
      "Templates"
    ]
  },
  {
    "objectID": "toolkit_blog/first_blog/index.html",
    "href": "toolkit_blog/first_blog/index.html",
    "title": "Let’s talk data privacy",
    "section": "",
    "text": "I wanted to explore a few issues of data privacy as they affect your average prospect development, management, or research professional.\nThis toolkit, at least in the alpha stages, will be focused on tools that are available / compliant in the USA, simply because that’s where I am based. Some resources mentioned here may not be available internationally, or may be subject to stricter privacy laws.\nProspect researchers take ethics very seriously, and some of the tools or methods that are available in America - or some of the more experimental proactive research I prototype in this toolkit - may not be appropriate for every organization.\nThese days, donor trust on an institutional level can be hard to gain and easy to lose. We exist in a difficult data privacy environment, where many competing interests are involved. I once had a job where leadership jokingly started referring to us prospect research and dev folks as “stalkers”, including, to my horror, in the presence of donors that happened to be touring the building.\nI had to eventually step up in a meeting and attempt to explain that it was really a short-sighted way of getting a laugh, considering the damage it could cause to morale and with donor relations. Being a lowly analyst, I tried to be diplomatic - and hopefully succeeded. But the incident stands out in my mind as a good example of the natural tension that exists in prospect research: we need to be experts on donors, which means having an expert handle on donor data. We must use our expertise that complies with institutional bylaws, legal requirements, and the standards spelled out by our professional organizations.\nEven when we are doing our best, however, those efforts may be perceived as invasive or inappropriate. It’s easy to reframe the conversation with ourselves, of course, because we believe in the mission of our organization. Philanthropy benefits many stakeholders, but we also exist in a competitive funding environment, which means we have to chase down leads to a certain extent - before passing them on to front-line fundraisers.\nStill, there is a natural tension in our jobs - and one that merits discussion every year, as technology and laws change.\n\n\nIn May 2025, LexisNexis had a data breach that originated from an attacker accessing their Github account. Data breaches are unfortunately common, but they are also a smaller issue compared to the larger questions they tend to evoke: what info IS ethical to gather about individuals? Just because something is available, does not mean that it is ethical to use… of course, there is an interesting wrinkle to the digital ecosystem in which prospect researchers attempt to apply ethics: data brokers are not just vulnerable to breaches, they actively purchase or scrape breach data themselves.\nBreach data is sometimes considered “public”, but that definition is used loosely by companies, similar to how AI companies have scraped both copywrighted works and public items on the internet - through a variety of means - with the argument that their final product is transformative enough to dodge legal challenges.\nOne fascinating aspect of the fundraising industry is the fact that many of us tend to use tools (like LexisNexis) that live somewhat in the heart of an unfolding controversy - an ongoing worldwide conversation that can get quite heated. Don’t get me wrong, I love using LexisNexis, but it’s important to understand the technology we use, and to vet how data is curated. Ultimately, we don’t have much power as analysts to impact these discussions, but it can help us gain fidelity in our understanding of the waters in which we swim - and when there is less mystery, even if we don’t always like what we find, I think it can give a measure of confidence when making decisions.\nThere are countless round tables and think pieces and meetings about compliance, of course, that dig deeper into these issues - but I think it’s important to point out the tensions or stressors that exist in a job.\n\n\n\nIt’s hard to be a circumspect prospect researcher and abide by the ethos of “data minimalism” - the minimum viable amount of data to complete a research project on a prospective donor - when we swim in an ocean of tools and companies that rely on data maximalism as their core business model.\n\n\n\nFor prospect researchers, they may feel conflicted about using AI tools or platforms that collate information from data brokers with black box methods that are subject to legal scrutiny (e.g. data privacy laws in California, or Europe).\nResearch is not always a cold, logical, straightforward deal. I mean, when I first started as a prospect development analyst…I would find that reading too many obituaries in one day would make me feel kinda rough, especially if they were for individuals near my own age.\nWhat we research, and how we research, can have downstream effects on our moods– positive or negative. Curiosity, for example, is an example of a positive trait that I think can accompany positive moods while researching - research can be quite fun! Alternatively, getting bogged down in a research rabbit hole without finding the answer to your key research question…while competing deadlines threaten to derail the session…not fun.\nSome folks are better at managing the “research posture”, the way one mentally approaches the job, and I think it’s one of the key skills that blends both hard and soft skills.\nWhat does this have to do with data privacy? Well, being compliant and training in ethics affects our research posture and how we react to different tools, use-cases, requests, or situations.\nIn some cases, you may be put in a position where you are requested to find information about a donor that crosses a line - a request that often happens by accident - and it’s important to understand your job well enough to educate others, even your supervisors sometimes, about the lines in the sand."
  },
  {
    "objectID": "toolkit_blog/first_blog/index.html#data-breaches",
    "href": "toolkit_blog/first_blog/index.html#data-breaches",
    "title": "Let’s talk data privacy",
    "section": "",
    "text": "In May 2025, LexisNexis had a data breach that originated from an attacker accessing their Github account. Data breaches are unfortunately common, but they are also a smaller issue compared to the larger questions they tend to evoke: what info IS ethical to gather about individuals? Just because something is available, does not mean that it is ethical to use… of course, there is an interesting wrinkle to the digital ecosystem in which prospect researchers attempt to apply ethics: data brokers are not just vulnerable to breaches, they actively purchase or scrape breach data themselves.\nBreach data is sometimes considered “public”, but that definition is used loosely by companies, similar to how AI companies have scraped both copywrighted works and public items on the internet - through a variety of means - with the argument that their final product is transformative enough to dodge legal challenges.\nOne fascinating aspect of the fundraising industry is the fact that many of us tend to use tools (like LexisNexis) that live somewhat in the heart of an unfolding controversy - an ongoing worldwide conversation that can get quite heated. Don’t get me wrong, I love using LexisNexis, but it’s important to understand the technology we use, and to vet how data is curated. Ultimately, we don’t have much power as analysts to impact these discussions, but it can help us gain fidelity in our understanding of the waters in which we swim - and when there is less mystery, even if we don’t always like what we find, I think it can give a measure of confidence when making decisions.\nThere are countless round tables and think pieces and meetings about compliance, of course, that dig deeper into these issues - but I think it’s important to point out the tensions or stressors that exist in a job."
  },
  {
    "objectID": "toolkit_blog/first_blog/index.html#data-minimalism-vs.-data-maximalism",
    "href": "toolkit_blog/first_blog/index.html#data-minimalism-vs.-data-maximalism",
    "title": "Let’s talk data privacy",
    "section": "",
    "text": "It’s hard to be a circumspect prospect researcher and abide by the ethos of “data minimalism” - the minimum viable amount of data to complete a research project on a prospective donor - when we swim in an ocean of tools and companies that rely on data maximalism as their core business model."
  },
  {
    "objectID": "toolkit_blog/first_blog/index.html#research-and-emotions",
    "href": "toolkit_blog/first_blog/index.html#research-and-emotions",
    "title": "Let’s talk data privacy",
    "section": "",
    "text": "For prospect researchers, they may feel conflicted about using AI tools or platforms that collate information from data brokers with black box methods that are subject to legal scrutiny (e.g. data privacy laws in California, or Europe).\nResearch is not always a cold, logical, straightforward deal. I mean, when I first started as a prospect development analyst…I would find that reading too many obituaries in one day would make me feel kinda rough, especially if they were for individuals near my own age.\nWhat we research, and how we research, can have downstream effects on our moods– positive or negative. Curiosity, for example, is an example of a positive trait that I think can accompany positive moods while researching - research can be quite fun! Alternatively, getting bogged down in a research rabbit hole without finding the answer to your key research question…while competing deadlines threaten to derail the session…not fun.\nSome folks are better at managing the “research posture”, the way one mentally approaches the job, and I think it’s one of the key skills that blends both hard and soft skills.\nWhat does this have to do with data privacy? Well, being compliant and training in ethics affects our research posture and how we react to different tools, use-cases, requests, or situations.\nIn some cases, you may be put in a position where you are requested to find information about a donor that crosses a line - a request that often happens by accident - and it’s important to understand your job well enough to educate others, even your supervisors sometimes, about the lines in the sand."
  },
  {
    "objectID": "toolkit_blog/technical_field_guide/index.html",
    "href": "toolkit_blog/technical_field_guide/index.html",
    "title": "Technical Field Guide",
    "section": "",
    "text": "…"
  },
  {
    "objectID": "toolkit_blog/technical_field_guide/index.html#section-i-getting-started-in-prospect-research",
    "href": "toolkit_blog/technical_field_guide/index.html#section-i-getting-started-in-prospect-research",
    "title": "Technical Field Guide",
    "section": "Section I: Getting Started in Prospect Research",
    "text": "Section I: Getting Started in Prospect Research\n\nChapter 1: Trailheads — Career Paths in Prospect Research\n\nDay-to-day realities\nEntry points and growth paths\nSkills and specializations\nOrg charts & responsibilities\n\n\n\nChapter 2: Mapping the Terrain — Tools of the Trade\n\nOverview of key platforms\nFree vs. paid resources\nWorkflow impact"
  },
  {
    "objectID": "toolkit_blog/technical_field_guide/index.html#section-ii-managing-a-research-shop",
    "href": "toolkit_blog/technical_field_guide/index.html#section-ii-managing-a-research-shop",
    "title": "Technical Field Guide",
    "section": "Section II: Managing a Research Shop",
    "text": "Section II: Managing a Research Shop\n\nChapter 3: Your New Shop\n\nProspect management vs. Prospect research vs. Data team\nProject management + project management systems\n\n\n\nChapter 4: Hiring, Interviews, Soft Skills\n\nHiring\nTechnical interviews\nCommunication\n\n\n\nChapter 5: Professional Development\n\nTraining\nProfessional associations and conferences\nCerts and degrees\n\n\n\nChapter 6: Compliance & Legal Landscape\n\nFor different niches\nEthics"
  },
  {
    "objectID": "toolkit_blog/technical_field_guide/index.html#section-iii-classic-research-strategies",
    "href": "toolkit_blog/technical_field_guide/index.html#section-iii-classic-research-strategies",
    "title": "Technical Field Guide",
    "section": "Section III: Classic Research Strategies",
    "text": "Section III: Classic Research Strategies\n\nChapter 7: Foundation Research — Mining 990s\n\nUnderstanding Form 990s\nUsing Foundation Directory Online and similar tools\nIdentifying institutional giving trends\nEvaluating foundation capacity and priorities\n\n\n\nChapter 8: Company Research and Executive Compensation\n\nPublic vs. private company data\nSources for executive pay and equity holdings\nEvaluating corporate affiliations and influence\nRed flags and wealth indicators\n\n\n\nChapter 9: Writing Profiles\n\n…\n\n\n\nChapter 10: Handling Research Requests\n\n…"
  },
  {
    "objectID": "toolkit_blog/technical_field_guide/index.html#section-iv-software-tools-and-platforms",
    "href": "toolkit_blog/technical_field_guide/index.html#section-iv-software-tools-and-platforms",
    "title": "Technical Field Guide",
    "section": "Section IV: Software Tools and Platforms",
    "text": "Section IV: Software Tools and Platforms\n\nChapter 11: CRMs - Navigating the Big Systems\n\nSalesforce, Blackbaud, Ellucian\nIntegrating research data\nReporting and collaboration\n\n\n\nChapter 12: Prospectin’ with LinkedIn Sales Navigator\n\nDonor discovery strategies\nPersona building by title, geography, seniority\nLimitations and practical tips\n\n\n\nChapter 13: Lexis Nexis — Deep Dives into Public Records\n\nWealth indicators\nLegal and business data\nEthical considerations\n\n\n\nChapter 14: iWave — Multi-Source Prospecting\n\nCapacity ratings\nReal estate, donations, and affiliations\nCustom scoring models\n\n\n\nChapter 15: AI Agents and LLMs — The New Frontier\n\nUse of generative AI in research\nAutomating tasks and summarization\nRisks, accuracy, and ethical use\n\n\n\nChapter 16: Free Alternatives for Small Shops / Solo Researchers\n\nOpen-source software\nLimitations and opportunities\n\n\n\nChapter 17: Building a Proactive Alerts-Driven Observatory\n\nPlatform-specific alerts\nSifting strategies\nWorkflow, alert management"
  },
  {
    "objectID": "toolkit_blog/technical_field_guide/index.html#section-v-data-management-analytics-and-it-infrastructure",
    "href": "toolkit_blog/technical_field_guide/index.html#section-v-data-management-analytics-and-it-infrastructure",
    "title": "Technical Field Guide",
    "section": "Section V: Data Management, Analytics, and IT Infrastructure",
    "text": "Section V: Data Management, Analytics, and IT Infrastructure\n\nChapter 18: Data Integrity and Hygiene — Keeping the Camp Clean\n\nCommon data issues\nMaintenance workflows\nGovernance and documentation\n\n\n\nChapter 19: Information Science — When You’re the One-Stop Shop\n\nOrganizing and retrieving data\nMetadata and taxonomy basics\nBuilding internal knowledge systems\nIT systems\n\n\n\nChapter 20: Analytics\n\nDashboards\nMetrics\n\n\n\nChapter 21: Personas and Affinity Scores — Profiling the Donor Landscape\n\nAffinity score construction\nDonor personas and use cases\nData challenges and edge cases\nExperiential, communication, philanthropic, volunteer categories\nTurning engagement into insights\nData engineering challenges"
  },
  {
    "objectID": "toolkit_blog/technical_field_guide/index.html#section-vi-case-studies",
    "href": "toolkit_blog/technical_field_guide/index.html#section-vi-case-studies",
    "title": "Technical Field Guide",
    "section": "Section VI: Case Studies",
    "text": "Section VI: Case Studies\n\nChapter 22: Higher Education\n\n\nChapter 23: Healthcare\n\n\nChapter 24: Other Niches"
  },
  {
    "objectID": "toolkit_tutorials/CodeBlockTesting/index.html",
    "href": "toolkit_tutorials/CodeBlockTesting/index.html",
    "title": "Analysis of Cars - an example from quarto",
    "section": "",
    "text": "Testing out using code chunks"
  },
  {
    "objectID": "toolkit_tutorials/CodeBlockTesting/index.html#sub-header",
    "href": "toolkit_tutorials/CodeBlockTesting/index.html#sub-header",
    "title": "Analysis of Cars - an example from quarto",
    "section": "Sub-Header",
    "text": "Sub-Header\nHere is a plot:\n\nlibrary(tidyverse)\ndata(mtcars)\nmtcars |&gt;\n  ggplot(aes(mpg, disp)) + \n  geom_point(color=\"dodgerblue4\", size=2)"
  },
  {
    "objectID": "tools/free_tools/free_external/index.html",
    "href": "tools/free_tools/free_external/index.html",
    "title": "Free External Resources",
    "section": "",
    "text": "The Prospect Research Institute has free resources available, but some of them are out of date, since their focus is currently on creating paid training with clients. See some research specific pages here: Prospect Research.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "tools.html#free-tools",
    "href": "tools.html#free-tools",
    "title": "Tools",
    "section": "Free tools",
    "text": "Free tools"
  },
  {
    "objectID": "tools.html#paid-tools",
    "href": "tools.html#paid-tools",
    "title": "Tools",
    "section": "Paid tools",
    "text": "Paid tools"
  }
]